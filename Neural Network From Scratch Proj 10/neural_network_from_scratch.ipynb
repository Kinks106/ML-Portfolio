{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab6da693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2aa728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./train.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4dd0c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)\n",
    "\n",
    "m, n = data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5002fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev = data[:1000].T\n",
    "Y_dev = data_dev[0].astype(int)\n",
    "X_dev = data_dev[1:n] / 255.0\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0].astype(int)\n",
    "X_train = data_train[1:n] / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dec9aa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (784, 41000) Labels: (41000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape:\", X_train.shape, \"Labels:\", Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0ffd57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    w1 = np.random.randn(10, 784) * np.sqrt(2 / 784)\n",
    "    b1 = np.zeros((10, 1))\n",
    "    w2 = np.random.randn(10, 10) * np.sqrt(2 / 10)\n",
    "    b2 = np.zeros((10, 1))\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "def ReLu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=0, keepdims=True)\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "def forward_prop(w1, b1, w2, b2, x):\n",
    "    z1 = w1.dot(x) + b1\n",
    "    a1 = ReLu(z1)\n",
    "    z2 = w2.dot(a1) + b2\n",
    "    a2 = softmax(z2)\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "def one_hot(y):\n",
    "    one_hot_y = np.zeros((y.size, y.max() + 1))\n",
    "    one_hot_y[np.arange(y.size), y] = 1\n",
    "    return one_hot_y.T\n",
    "\n",
    "def deriv_Relu(z):\n",
    "    return z > 0\n",
    "\n",
    "def back_prop(z1, a1, z2, a2, w2, x, y):\n",
    "    m = y.size\n",
    "    one_hot_y = one_hot(y)\n",
    "    dz2 = a2 - one_hot_y\n",
    "    dw2 = (1/m) * dz2.dot(a1.T)\n",
    "    db2 = (1/m) * np.sum(dz2, axis=1, keepdims=True)\n",
    "    dz1 = w2.T.dot(dz2) * deriv_Relu(z1)\n",
    "    dw1 = (1/m) * dz1.dot(x.T)\n",
    "    db1 = (1/m) * np.sum(dz1, axis=1, keepdims=True)\n",
    "    return dw1, db1, dw2, db2\n",
    "\n",
    "def update_params(dw1, db1, dw2, db2, w1, b1, w2, b2, alpha):\n",
    "    w1 -= alpha * dw1\n",
    "    b1 -= alpha * db1\n",
    "    w2 -= alpha * dw2\n",
    "    b2 -= alpha * db2\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4c2f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(a2):\n",
    "    return np.argmax(a2, axis=0)\n",
    "\n",
    "def get_accuracy(predictions, y):\n",
    "    return np.mean(predictions == y)\n",
    "\n",
    "def compute_loss(a2, y):\n",
    "    m = y.size\n",
    "    log_likelihood = -np.log(a2[y, np.arange(m)] + 1e-9)\n",
    "    return np.sum(log_likelihood) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "39a3e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, iterations, alpha):\n",
    "    w1, b1, w2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        z1, a1, z2, a2 = forward_prop(w1, b1, w2, b2, x)\n",
    "        dw1, db1, dw2, db2 = back_prop(z1, a1, z2, a2, w2, x, y)\n",
    "        w1, b1, w2, b2 = update_params(dw1, db1, dw2, db2, w1, b1, w2, b2, alpha)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            predictions = get_predictions(a2)\n",
    "            accuracy = get_accuracy(predictions, y)\n",
    "            loss = compute_loss(a2, y)\n",
    "            print(f\"Iteration {i}: Accuracy = {accuracy:.4f}, Loss = {loss:.4f}\")\n",
    "\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "793681ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Accuracy = 0.1351, Loss = 2.3728\n",
      "Iteration 10: Accuracy = 0.1511, Loss = 2.2791\n",
      "Iteration 20: Accuracy = 0.1677, Loss = 2.2264\n",
      "Iteration 30: Accuracy = 0.1884, Loss = 2.1814\n",
      "Iteration 40: Accuracy = 0.2154, Loss = 2.1348\n",
      "Iteration 50: Accuracy = 0.2463, Loss = 2.0840\n",
      "Iteration 60: Accuracy = 0.2836, Loss = 2.0290\n",
      "Iteration 70: Accuracy = 0.3315, Loss = 1.9731\n",
      "Iteration 80: Accuracy = 0.3711, Loss = 1.9203\n",
      "Iteration 90: Accuracy = 0.3994, Loss = 1.8712\n",
      "Iteration 100: Accuracy = 0.4240, Loss = 1.8251\n",
      "Iteration 110: Accuracy = 0.4460, Loss = 1.7814\n",
      "Iteration 120: Accuracy = 0.4685, Loss = 1.7396\n",
      "Iteration 130: Accuracy = 0.4898, Loss = 1.6994\n",
      "Iteration 140: Accuracy = 0.5091, Loss = 1.6603\n",
      "Iteration 150: Accuracy = 0.5269, Loss = 1.6221\n",
      "Iteration 160: Accuracy = 0.5429, Loss = 1.5849\n",
      "Iteration 170: Accuracy = 0.5558, Loss = 1.5485\n",
      "Iteration 180: Accuracy = 0.5685, Loss = 1.5129\n",
      "Iteration 190: Accuracy = 0.5805, Loss = 1.4782\n",
      "Iteration 200: Accuracy = 0.5917, Loss = 1.4444\n",
      "Iteration 210: Accuracy = 0.6028, Loss = 1.4115\n",
      "Iteration 220: Accuracy = 0.6133, Loss = 1.3796\n",
      "Iteration 230: Accuracy = 0.6227, Loss = 1.3486\n",
      "Iteration 240: Accuracy = 0.6348, Loss = 1.3186\n",
      "Iteration 250: Accuracy = 0.6471, Loss = 1.2895\n",
      "Iteration 260: Accuracy = 0.6591, Loss = 1.2614\n",
      "Iteration 270: Accuracy = 0.6716, Loss = 1.2343\n",
      "Iteration 280: Accuracy = 0.6822, Loss = 1.2082\n",
      "Iteration 290: Accuracy = 0.6924, Loss = 1.1831\n",
      "Iteration 300: Accuracy = 0.7020, Loss = 1.1590\n",
      "Iteration 310: Accuracy = 0.7112, Loss = 1.1358\n",
      "Iteration 320: Accuracy = 0.7195, Loss = 1.1136\n",
      "Iteration 330: Accuracy = 0.7276, Loss = 1.0924\n",
      "Iteration 340: Accuracy = 0.7345, Loss = 1.0721\n",
      "Iteration 350: Accuracy = 0.7408, Loss = 1.0526\n",
      "Iteration 360: Accuracy = 0.7462, Loss = 1.0341\n",
      "Iteration 370: Accuracy = 0.7510, Loss = 1.0163\n",
      "Iteration 380: Accuracy = 0.7558, Loss = 0.9993\n",
      "Iteration 390: Accuracy = 0.7600, Loss = 0.9831\n",
      "Iteration 400: Accuracy = 0.7642, Loss = 0.9676\n",
      "Iteration 410: Accuracy = 0.7683, Loss = 0.9527\n",
      "Iteration 420: Accuracy = 0.7715, Loss = 0.9385\n",
      "Iteration 430: Accuracy = 0.7744, Loss = 0.9249\n",
      "Iteration 440: Accuracy = 0.7776, Loss = 0.9118\n",
      "Iteration 450: Accuracy = 0.7805, Loss = 0.8993\n",
      "Iteration 460: Accuracy = 0.7832, Loss = 0.8872\n",
      "Iteration 470: Accuracy = 0.7856, Loss = 0.8756\n",
      "Iteration 480: Accuracy = 0.7871, Loss = 0.8645\n",
      "Iteration 490: Accuracy = 0.7890, Loss = 0.8538\n",
      "Iteration 500: Accuracy = 0.7913, Loss = 0.8434\n",
      "Iteration 510: Accuracy = 0.7930, Loss = 0.8334\n",
      "Iteration 520: Accuracy = 0.7947, Loss = 0.8238\n",
      "Iteration 530: Accuracy = 0.7964, Loss = 0.8145\n",
      "Iteration 540: Accuracy = 0.7984, Loss = 0.8056\n",
      "Iteration 550: Accuracy = 0.8000, Loss = 0.7969\n",
      "Iteration 560: Accuracy = 0.8013, Loss = 0.7885\n",
      "Iteration 570: Accuracy = 0.8033, Loss = 0.7804\n",
      "Iteration 580: Accuracy = 0.8050, Loss = 0.7726\n",
      "Iteration 590: Accuracy = 0.8068, Loss = 0.7650\n",
      "Iteration 600: Accuracy = 0.8081, Loss = 0.7576\n",
      "Iteration 610: Accuracy = 0.8094, Loss = 0.7505\n",
      "Iteration 620: Accuracy = 0.8107, Loss = 0.7436\n",
      "Iteration 630: Accuracy = 0.8122, Loss = 0.7369\n",
      "Iteration 640: Accuracy = 0.8135, Loss = 0.7304\n",
      "Iteration 650: Accuracy = 0.8147, Loss = 0.7241\n",
      "Iteration 660: Accuracy = 0.8159, Loss = 0.7180\n",
      "Iteration 670: Accuracy = 0.8168, Loss = 0.7120\n",
      "Iteration 680: Accuracy = 0.8180, Loss = 0.7062\n",
      "Iteration 690: Accuracy = 0.8190, Loss = 0.7006\n",
      "Iteration 700: Accuracy = 0.8201, Loss = 0.6951\n",
      "Iteration 710: Accuracy = 0.8211, Loss = 0.6898\n",
      "Iteration 720: Accuracy = 0.8220, Loss = 0.6846\n",
      "Iteration 730: Accuracy = 0.8229, Loss = 0.6796\n",
      "Iteration 740: Accuracy = 0.8240, Loss = 0.6747\n",
      "Iteration 750: Accuracy = 0.8249, Loss = 0.6699\n",
      "Iteration 760: Accuracy = 0.8260, Loss = 0.6652\n",
      "Iteration 770: Accuracy = 0.8272, Loss = 0.6606\n",
      "Iteration 780: Accuracy = 0.8281, Loss = 0.6562\n",
      "Iteration 790: Accuracy = 0.8292, Loss = 0.6518\n",
      "Iteration 800: Accuracy = 0.8302, Loss = 0.6476\n",
      "Iteration 810: Accuracy = 0.8311, Loss = 0.6435\n",
      "Iteration 820: Accuracy = 0.8320, Loss = 0.6394\n",
      "Iteration 830: Accuracy = 0.8330, Loss = 0.6355\n",
      "Iteration 840: Accuracy = 0.8340, Loss = 0.6316\n",
      "Iteration 850: Accuracy = 0.8347, Loss = 0.6278\n",
      "Iteration 860: Accuracy = 0.8357, Loss = 0.6241\n",
      "Iteration 870: Accuracy = 0.8365, Loss = 0.6205\n",
      "Iteration 880: Accuracy = 0.8374, Loss = 0.6170\n",
      "Iteration 890: Accuracy = 0.8381, Loss = 0.6135\n",
      "Iteration 900: Accuracy = 0.8389, Loss = 0.6101\n",
      "Iteration 910: Accuracy = 0.8394, Loss = 0.6068\n",
      "Iteration 920: Accuracy = 0.8397, Loss = 0.6035\n",
      "Iteration 930: Accuracy = 0.8403, Loss = 0.6003\n",
      "Iteration 940: Accuracy = 0.8410, Loss = 0.5972\n",
      "Iteration 950: Accuracy = 0.8418, Loss = 0.5941\n",
      "Iteration 960: Accuracy = 0.8425, Loss = 0.5911\n",
      "Iteration 970: Accuracy = 0.8431, Loss = 0.5882\n",
      "Iteration 980: Accuracy = 0.8438, Loss = 0.5853\n",
      "Iteration 990: Accuracy = 0.8449, Loss = 0.5824\n",
      "Iteration 1000: Accuracy = 0.8455, Loss = 0.5797\n",
      "Iteration 1010: Accuracy = 0.8462, Loss = 0.5769\n",
      "Iteration 1020: Accuracy = 0.8470, Loss = 0.5742\n",
      "Iteration 1030: Accuracy = 0.8474, Loss = 0.5716\n",
      "Iteration 1040: Accuracy = 0.8482, Loss = 0.5690\n",
      "Iteration 1050: Accuracy = 0.8487, Loss = 0.5664\n",
      "Iteration 1060: Accuracy = 0.8492, Loss = 0.5639\n",
      "Iteration 1070: Accuracy = 0.8497, Loss = 0.5614\n",
      "Iteration 1080: Accuracy = 0.8503, Loss = 0.5590\n",
      "Iteration 1090: Accuracy = 0.8507, Loss = 0.5566\n",
      "Iteration 1100: Accuracy = 0.8511, Loss = 0.5543\n",
      "Iteration 1110: Accuracy = 0.8516, Loss = 0.5520\n",
      "Iteration 1120: Accuracy = 0.8520, Loss = 0.5497\n",
      "Iteration 1130: Accuracy = 0.8524, Loss = 0.5475\n",
      "Iteration 1140: Accuracy = 0.8529, Loss = 0.5453\n",
      "Iteration 1150: Accuracy = 0.8534, Loss = 0.5431\n",
      "Iteration 1160: Accuracy = 0.8542, Loss = 0.5410\n",
      "Iteration 1170: Accuracy = 0.8548, Loss = 0.5389\n",
      "Iteration 1180: Accuracy = 0.8552, Loss = 0.5368\n",
      "Iteration 1190: Accuracy = 0.8558, Loss = 0.5348\n",
      "Iteration 1200: Accuracy = 0.8562, Loss = 0.5328\n",
      "Iteration 1210: Accuracy = 0.8568, Loss = 0.5308\n",
      "Iteration 1220: Accuracy = 0.8570, Loss = 0.5288\n",
      "Iteration 1230: Accuracy = 0.8577, Loss = 0.5269\n",
      "Iteration 1240: Accuracy = 0.8582, Loss = 0.5250\n",
      "Iteration 1250: Accuracy = 0.8584, Loss = 0.5232\n",
      "Iteration 1260: Accuracy = 0.8588, Loss = 0.5213\n",
      "Iteration 1270: Accuracy = 0.8592, Loss = 0.5195\n",
      "Iteration 1280: Accuracy = 0.8598, Loss = 0.5177\n",
      "Iteration 1290: Accuracy = 0.8600, Loss = 0.5160\n",
      "Iteration 1300: Accuracy = 0.8604, Loss = 0.5142\n",
      "Iteration 1310: Accuracy = 0.8611, Loss = 0.5125\n",
      "Iteration 1320: Accuracy = 0.8616, Loss = 0.5108\n",
      "Iteration 1330: Accuracy = 0.8619, Loss = 0.5092\n",
      "Iteration 1340: Accuracy = 0.8622, Loss = 0.5075\n",
      "Iteration 1350: Accuracy = 0.8626, Loss = 0.5059\n",
      "Iteration 1360: Accuracy = 0.8629, Loss = 0.5043\n",
      "Iteration 1370: Accuracy = 0.8632, Loss = 0.5027\n",
      "Iteration 1380: Accuracy = 0.8636, Loss = 0.5011\n",
      "Iteration 1390: Accuracy = 0.8640, Loss = 0.4996\n",
      "Iteration 1400: Accuracy = 0.8643, Loss = 0.4981\n",
      "Iteration 1410: Accuracy = 0.8648, Loss = 0.4966\n",
      "Iteration 1420: Accuracy = 0.8653, Loss = 0.4951\n",
      "Iteration 1430: Accuracy = 0.8656, Loss = 0.4936\n",
      "Iteration 1440: Accuracy = 0.8660, Loss = 0.4922\n",
      "Iteration 1450: Accuracy = 0.8661, Loss = 0.4907\n",
      "Iteration 1460: Accuracy = 0.8665, Loss = 0.4893\n",
      "Iteration 1470: Accuracy = 0.8669, Loss = 0.4879\n",
      "Iteration 1480: Accuracy = 0.8673, Loss = 0.4865\n",
      "Iteration 1490: Accuracy = 0.8679, Loss = 0.4852\n",
      "Iteration 1500: Accuracy = 0.8681, Loss = 0.4838\n",
      "Iteration 1510: Accuracy = 0.8684, Loss = 0.4825\n",
      "Iteration 1520: Accuracy = 0.8687, Loss = 0.4812\n",
      "Iteration 1530: Accuracy = 0.8692, Loss = 0.4799\n",
      "Iteration 1540: Accuracy = 0.8695, Loss = 0.4786\n",
      "Iteration 1550: Accuracy = 0.8698, Loss = 0.4774\n",
      "Iteration 1560: Accuracy = 0.8702, Loss = 0.4761\n",
      "Iteration 1570: Accuracy = 0.8704, Loss = 0.4749\n",
      "Iteration 1580: Accuracy = 0.8706, Loss = 0.4736\n",
      "Iteration 1590: Accuracy = 0.8708, Loss = 0.4724\n",
      "Iteration 1600: Accuracy = 0.8710, Loss = 0.4712\n",
      "Iteration 1610: Accuracy = 0.8714, Loss = 0.4700\n",
      "Iteration 1620: Accuracy = 0.8718, Loss = 0.4689\n",
      "Iteration 1630: Accuracy = 0.8721, Loss = 0.4677\n",
      "Iteration 1640: Accuracy = 0.8725, Loss = 0.4666\n",
      "Iteration 1650: Accuracy = 0.8727, Loss = 0.4654\n",
      "Iteration 1660: Accuracy = 0.8730, Loss = 0.4643\n",
      "Iteration 1670: Accuracy = 0.8734, Loss = 0.4632\n",
      "Iteration 1680: Accuracy = 0.8738, Loss = 0.4621\n",
      "Iteration 1690: Accuracy = 0.8740, Loss = 0.4610\n",
      "Iteration 1700: Accuracy = 0.8743, Loss = 0.4599\n",
      "Iteration 1710: Accuracy = 0.8744, Loss = 0.4589\n",
      "Iteration 1720: Accuracy = 0.8747, Loss = 0.4578\n",
      "Iteration 1730: Accuracy = 0.8749, Loss = 0.4568\n",
      "Iteration 1740: Accuracy = 0.8751, Loss = 0.4558\n",
      "Iteration 1750: Accuracy = 0.8753, Loss = 0.4547\n",
      "Iteration 1760: Accuracy = 0.8755, Loss = 0.4537\n",
      "Iteration 1770: Accuracy = 0.8757, Loss = 0.4527\n",
      "Iteration 1780: Accuracy = 0.8760, Loss = 0.4517\n",
      "Iteration 1790: Accuracy = 0.8762, Loss = 0.4507\n",
      "Iteration 1800: Accuracy = 0.8763, Loss = 0.4498\n",
      "Iteration 1810: Accuracy = 0.8765, Loss = 0.4488\n",
      "Iteration 1820: Accuracy = 0.8767, Loss = 0.4479\n",
      "Iteration 1830: Accuracy = 0.8769, Loss = 0.4469\n",
      "Iteration 1840: Accuracy = 0.8772, Loss = 0.4460\n",
      "Iteration 1850: Accuracy = 0.8774, Loss = 0.4451\n",
      "Iteration 1860: Accuracy = 0.8776, Loss = 0.4442\n",
      "Iteration 1870: Accuracy = 0.8778, Loss = 0.4432\n",
      "Iteration 1880: Accuracy = 0.8780, Loss = 0.4423\n",
      "Iteration 1890: Accuracy = 0.8782, Loss = 0.4415\n",
      "Iteration 1900: Accuracy = 0.8785, Loss = 0.4406\n",
      "Iteration 1910: Accuracy = 0.8789, Loss = 0.4397\n",
      "Iteration 1920: Accuracy = 0.8791, Loss = 0.4388\n",
      "Iteration 1930: Accuracy = 0.8793, Loss = 0.4380\n",
      "Iteration 1940: Accuracy = 0.8795, Loss = 0.4371\n",
      "Iteration 1950: Accuracy = 0.8796, Loss = 0.4363\n",
      "Iteration 1960: Accuracy = 0.8799, Loss = 0.4355\n",
      "Iteration 1970: Accuracy = 0.8803, Loss = 0.4346\n",
      "Iteration 1980: Accuracy = 0.8806, Loss = 0.4338\n",
      "Iteration 1990: Accuracy = 0.8807, Loss = 0.4330\n"
     ]
    }
   ],
   "source": [
    "w1, b1, w2, b2 = gradient_descent(X_train, Y_train, 2000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56d0ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    predictions = get_predictions(A2)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def test_prediction(index, X_data, Y_data, W1, b1, W2, b2):\n",
    "    # Extract single sample (column vector)\n",
    "    current_image = X_data[:, index, None]\n",
    "    \n",
    "    # Model prediction\n",
    "    prediction = make_predictions(current_image, W1, b1, W2, b2)[0]\n",
    "    label = Y_data[index]\n",
    "    \n",
    "    print(f\"ðŸ§  Prediction: {prediction}\")\n",
    "    print(f\"âœ… Actual Label: {label}\")\n",
    "    \n",
    "    # Reshape and scale back to 0â€“255 for visualization\n",
    "    current_image = (X_data[:, index].reshape(28, 28) * 255).astype(np.uint8)\n",
    "    \n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(current_image, cmap='gray', interpolation='nearest')\n",
    "    plt.title(f\"Predicted: {prediction} | Actual: {label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42295bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Prediction: 7\n",
      "âœ… Actual Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_prediction(3, X_dev, Y_dev, w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e1369dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.854)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_predictions = make_predictions(X_dev, w1, b1, w2, b2)\n",
    "get_accuracy(dev_predictions, Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e320989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
